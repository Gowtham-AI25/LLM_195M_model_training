# =========================================================
# LLM Training Configuration
# =========================================================

# =========================
# 1. Training Loop
# =========================
total_training_steps: 500
accumulation_steps: 5
max_grad_norm: 1.0

# Device & Distributed
device: cuda
num_devices: 2

# =========================
# 2. Optimizer
# =========================
optimizer_type: AdamW
learning_rate: 2.0e-4
weight_decay: 0.01

beta1: 0.9
beta2: 0.95
optim_eps: 1.0e-8

# =========================
# 3. LR Scheduler (4-phase)
# =========================
scheduler_type: CosineAnnealingWithWarmup
min_lr_ratio: 0.1

warmup_steps: 25
plateau_steps: 50
decay_steps: 400
anneal_steps: 25

# =========================
# 4. Data Loader
# =========================

block_size: 1024
batch_size: 4
num_workers: 4
pin_memory: true

# =========================
# 5. AMP /  Precision
# =========================
use_amp: true
dtype: float16

# =========================
# 6. Logging & Checkpointing
# =========================
log_every_steps: 1
checkpoint_every_steps: 50
eval_every_steps: 100

# ========================= 
# 7. Paths
# =========================
checkpoint_path: "llm_training_project/checkpoints_dir/checkpoints/checkpoint.pt"
dataset_dir: "llm_training_project/shards"
checkpoint_dir: "llm_training_project/checkpoints_dir/"

shard_manager_json_path: "llm_training_project/shards/shard_files.json"
tensorboard_log_dir: "llm_training_project/log/tb_logs"

