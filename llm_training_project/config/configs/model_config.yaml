# -------------------------
# Tokenizer / Embedding
# -------------------------
vocab_size: 32000
emb_dim: 1024                
padding_idx: 0
max_seq_len: 1024
tie_embeddings: true         # Essential to fit 16 layers at this width

# -------------------------
# Transformer Depth
# -------------------------
n_blocks: 20                 # Increased depth for better reasoning

# -------------------------
# Attention (GQA 16/8)
# -------------------------
num_q_heads: 16             
num_kv_heads: 8              
rope_base: 10000.0

# -------------------------
# FFN (SwiGLU)
# -------------------------
# Formula: 3 * emb_dim * ffn_hidden_dim
ffn_hidden_dim: 2752         

# -------------------------
# Stability / Optimization
# -------------------------
eps: 1.0e-6
dropout_rate: 0.1
bias: false
