# -------------------------
# Tokenizer / Embedding
# -------------------------
vocab_size: 32000
emb_dim: 864            # Fixed: 864 is exactly divisible by 12 (864 / 12 = 72)
padding_idx: 0
max_seq_len: 1024       # Set to 1024 as requested
tie_embeddings: true    # Weight tying enabled to save params for depth

# -------------------------
# Transformer Depth
# -------------------------
n_blocks: 18            # 18 blocks + 864 width hits the ~195M param target perfectly

# -------------------------
# Attention (GQA)
# -------------------------
num_q_heads: 12         # As requested
num_kv_heads: 6         # As requested
rope_base: 10000.0

# -------------------------
# FFN (SwiGLU)
# -------------------------
ffn_hidden_dim: 2592    # Scaled to maintain the 195M parameter budget

# -------------------------
# Stability / Optimization
# -------------------------
eps: 1.0e-6
dropout_rate: 0.1
bias: false

# -------------------------
# Compile
# -------------------------
compile_model: false
