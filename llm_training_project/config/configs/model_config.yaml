# -------------------------
# Tokenizer / Embedding
# -------------------------
vocab_size: 32000
emb_dim: 832              # Slightly reduced width (compute-efficient)
padding_idx: 0
max_seq_len: 1024         # As requested
tie_embeddings: true

# -------------------------
# Transformer Depth
# -------------------------
n_blocks: 18              # Deeper > wider for 195M at shorter context

# -------------------------
# Attention (GQA)
# -------------------------
num_q_heads: 12
num_kv_heads: 6
rope_base: 10000.0

# -------------------------
# FFN (SwiGLU)
# -------------------------
ffn_hidden_dim: 2816      # ≈ 3.38 × emb_dim (compensates GQA + weight tying)

# -------------------------
# Stability / Optimization
# -------------------------
eps: 1.0e-6
dropout_rate: 0.1
bias: false

# -------------------------
# Compile
# -------------------------
compile_model: false
